<p>When you first encounter them, ROC&nbsp;Curve and AUC (area under curve) metrics can be a little confusing. But not to worry, with a little practice, they'll start to make sense.</p><p>In a nutshell, what you should remember is:</p><ul><li><p>ROC curves and AUC metrics are evaluation metrics for binary classification models (a model which predicts one thing or another, such as heart disease or not).</p></li><li><p>The ROC curve compares the true positive rate (tpr) versus the false positive rate (fpr) at different classification thresholds.</p></li><li><p>The AUC metric tells you how well your model is at choosing between classes (for example, how well it is at deciding whether someone has heart disease or not). A perfect model will get an AUC score of 1.</p></li></ul><p>For more information on these metrics, bookmark the following resources and refer to them when you need:</p><ul><li><p><a href="https://www.youtube.com/watch?v=4jRBRDbJemM" rel="noopener noreferrer" target="_blank">ROC and AUC, Clearly Explained!</a> by StatQuest</p></li><li><p><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html" rel="noopener noreferrer" target="_blank">ROC&nbsp;documentation in Scikit-Learn</a> (contains code examples)</p></li><li><p><a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc" rel="noopener noreferrer" target="_blank">How the ROC&nbsp;curve and AUC&nbsp;are calculated</a> by Google's Machine Learning team</p></li></ul>